{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building my Reddit Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have annotated my webscraper below, and commented out the cells that interact with the actual CSV that contains the data, since the python script that collects my data edits and rewrites that CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Without saying the name, what is a TV show that is instantly recognizable based on one quote?\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up the driver\n",
    "\n",
    "ser = Service('../chromedriver/chromedriver.exe')\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=ser, options=op)\n",
    "driver.get('https://www.reddit.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the previously scraped data\n",
    "# old_data = pd.read_csv('../data/reddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing the html for each post\n",
    "post = driver.find_element_by_tag_name('body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually scrolling reddit using page down 500 times\n",
    "count = 0\n",
    "while count < 500:\n",
    "    post.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the html from the scrolled page and create a soup    \n",
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing all posts\n",
    "all_posts = soup.findAll('div',{'class':'_1oQyIsiPHYt6nx7VOmd1sz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the current time to be saved with each scrape\n",
    "current_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of each individual post\n",
    "post_list = []\n",
    "for post in all_posts:\n",
    " \n",
    "    post_dict = {}\n",
    "    try:    \n",
    "        #if this first line of code throws an error, post does not contain\n",
    "        #image, goes on to next block\n",
    "        post.find('img',{'alt':'Post image'}).get('src')\n",
    "        #append the relevant information to our list of dictionaries\n",
    "        post_list.append({\n",
    "        #get the unique post id\n",
    "        'id':post.get('id'),\n",
    "        #get the post title\n",
    "        'title':post.find('h3', {'class':'_eYtD2XCVieq6emjKBH3m'}).text,\n",
    "        #get the number of comments\n",
    "        'comments':post.find(\"span\", {'class': 'FHCV02u6Cp2zYL0fhQPsO'}).text,\n",
    "        #find which subreddit post was posted to\n",
    "        'subreddit':(post.find('a', {\n",
    "            'class':'_3ryJoIoycVkA88fy40qNJc'})).get('href'),\n",
    "        #get how many hours ago it was posted\n",
    "        'time':post.find('a',{'class':'_3jOxDPIQ0KaOWpzvSQo-1s'}).text,\n",
    "        #get the number of upvotes\n",
    "        'upvotes':post.find('div', {\n",
    "            'class':'_1rZYMD_4xY3gRcSS3p8ODO _3a2ZHWaih05DgAOtvu6cIo'}).text,\n",
    "        #note what time we scraped the data\n",
    "        'scraped_time':current_time,\n",
    "        #if the first line of code doesn't throw an error, post contains an\n",
    "        #image so we mark it as such below\n",
    "        'contains_image':'1',\n",
    "        'contains_video':'0',\n",
    "        'text_only':'0'\n",
    "        })\n",
    "    except:\n",
    "        try:\n",
    "            #if this line of code throws an error, then post also does not \n",
    "            #contain video, goes on to next block\n",
    "            post.find('video',{'class':'_1EQJpXY7ExS04odI1YBBlj'}).get('src')\n",
    "            #append the relevant information to our list of dictionaries\n",
    "            post_list.append({\n",
    "            'id':post.get('id'),\n",
    "            'title':post.find('h3', {'class':'_eYtD2XCVieq6emjKBH3m'}).text,\n",
    "            'comments':post.find(\"span\", {\n",
    "                'class': 'FHCV02u6Cp2zYL0fhQPsO'}).text,\n",
    "            'subreddit':(post.find('a', {\n",
    "                'class':'_3ryJoIoycVkA88fy40qNJc'})).get('href'),\n",
    "            'time':post.find('a',{'class':'_3jOxDPIQ0KaOWpzvSQo-1s'}).text,\n",
    "            'upvotes':post.find('div', {\n",
    "                'class':'_1rZYMD_4xY3gRcSS3p8ODO _3a2ZHWaih05DgAOtvu6cIo'}).text,\n",
    "            'scraped_time':current_time,\n",
    "            'contains_image':'0',\n",
    "            'contains_video':'1',\n",
    "            'text_only':'0'\n",
    "            })\n",
    "        except:\n",
    "            try:\n",
    "                #if THIS block of code throws an error, then the post is an ad, \n",
    "                #live stream, or live poll\n",
    "                #append the relevant information to our list of dictionaries\n",
    "                post_list.append({\n",
    "                'id':post.get('id'),\n",
    "                'title':post.find('h3', {'class':'_eYtD2XCVieq6emjKBH3m'}).text,\n",
    "                'comments':post.find(\"span\", {\n",
    "                    'class': 'FHCV02u6Cp2zYL0fhQPsO'}).text,\n",
    "                'subreddit':(post.find('a', {\n",
    "                    'class':'_3ryJoIoycVkA88fy40qNJc'})).get('href'),\n",
    "                'time':post.find('a',{'class':'_3jOxDPIQ0KaOWpzvSQo-1s'}).text,\n",
    "                'upvotes':post.find('div', {\n",
    "                    'class':'_1rZYMD_4xY3gRcSS3p8ODO _3a2ZHWaih05DgAOtvu6cIo'}).text,\n",
    "                'scraped_time':current_time,\n",
    "                'contains_image':'0',\n",
    "                'contains_video':'0',\n",
    "                'text_only':'1'\n",
    "                })\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe from our list of dictionaries\n",
    "new_data = pd.DataFrame(post_list)\n",
    "\n",
    "#combining the new data with what we have scraped previously\n",
    "combined_df = pd.concat([old_data, new_data])\n",
    "\n",
    "\n",
    "#DO NOT RUN THIS SMALL BLOCK OF CODE OR IT WILL OVERWRITE THE DATA\n",
    "# #saving df as csv\n",
    "# combined_df.to_csv('../data/reddit_data.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "#closing the driver\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
