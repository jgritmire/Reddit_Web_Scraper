{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0bb3db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:18.142127Z",
     "start_time": "2022-01-07T19:54:16.670065Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eee3958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:18.409412Z",
     "start_time": "2022-01-07T19:54:18.143126Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../data/reddit_data_cleaned.csv')\n",
    "df_tvec_10 = pd.read_csv('../data/title_tfidf_10.csv')\n",
    "df_tvec_25 = pd.read_csv('../data/title_tfidf_25.csv')\n",
    "df_tvec_50 = pd.read_csv('../data/title_tfidf_50.csv')\n",
    "df_tvec_100 = pd.read_csv('../data/title_tfidf_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6ac15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:18.441326Z",
     "start_time": "2022-01-07T19:54:18.410409Z"
    }
   },
   "outputs": [],
   "source": [
    "#Setting up a few dummy variables\n",
    "\n",
    "reddit = pd.get_dummies(reddit, columns=['posted_hour'])\n",
    "reddit = pd.get_dummies(reddit, columns=['posted_weekday'])\n",
    "# reddit = pd.get_dummies(reddit, columns=['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d010ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:18.537070Z",
     "start_time": "2022-01-07T19:54:18.442324Z"
    }
   },
   "outputs": [],
   "source": [
    "#merge the dataframes\n",
    "reddit_tfidf_10 = reddit.merge(df_tvec_10, on='post_id', how='left')\n",
    "#drop redundant columns\n",
    "reddit_tfidf_10.drop(columns = ['stemmed_title'], inplace = True)\n",
    "\n",
    "#merge the dataframes\n",
    "reddit_tfidf_25 = reddit.merge(df_tvec_25, on='post_id', how='left')\n",
    "#drop redundant columns\n",
    "reddit_tfidf_25.drop(columns = ['stemmed_title'], inplace = True)\n",
    "\n",
    "#merge the dataframes\n",
    "reddit_tfidf_50 = reddit.merge(df_tvec_50, on='post_id', how='left')\n",
    "#drop redundant columns\n",
    "reddit_tfidf_50.drop(columns = ['stemmed_title'], inplace = True)\n",
    "\n",
    "#merge the dataframes\n",
    "reddit_tfidf_100 = reddit.merge(df_tvec_100, on='post_id', how='left')\n",
    "#drop redundant columns\n",
    "reddit_tfidf_100.drop(columns = ['stemmed_title'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5d3f5",
   "metadata": {},
   "source": [
    "# Model Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394df375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:18.567988Z",
     "start_time": "2022-01-07T19:54:18.538069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5059046815689583"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit[reddit['target'] == 1])/len(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68609190",
   "metadata": {},
   "source": [
    "The baseline accuracy for my model is 50.14% (subject to change as I collect more data), if we predict that all posts are successful since there are slightly more successful posts that failed posts. For context, succesful is defined as getting above the median number of comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed19274",
   "metadata": {},
   "source": [
    "# RF Model Without TFIDF Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c80e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:22.376802Z",
     "start_time": "2022-01-07T19:54:18.568985Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7320479028656166 0.6904186551994136\n",
      "0.7112332790325151\n"
     ]
    }
   ],
   "source": [
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'stemmed_title','subreddit']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit.drop(columns = drop)\n",
    "y = reddit['target']\n",
    "\n",
    "#random forest model with 100 estimators\n",
    "my_forest = RandomForestClassifier(class_weight='balanced',\n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=10,\n",
    "                                  min_samples_split=50,                        \n",
    "                                  n_jobs=-1,\n",
    "                                  random_state = 48)\n",
    "\n",
    "score = cross_val_score(my_forest, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df10f7",
   "metadata": {},
   "source": [
    "After playing around with the parameters, I settled on a model that I believe will be less sususceptible to overfitting, with a relatively large number of min_samples_split and limiting max depth to 10. \n",
    "\n",
    "This model has a score of 0.71, with a 95% CI of between 0.73 and 0.68\n",
    "\n",
    "However, if we are asking the question: 'how do you make a successful reddit post' then it feels disingenuous to include upvotes as an independent variable, because as a content creator you have no control over the number of upvotes that your post is receiving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73dd8f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:24.235831Z",
     "start_time": "2022-01-07T19:54:22.381790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6100832293692197 0.5590420252044613\n",
      "0.5845626272868405\n"
     ]
    }
   ],
   "source": [
    "#RF model excluding 'upvotes' from X\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'stemmed_title','subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit.drop(columns = drop)\n",
    "y = reddit['target']\n",
    "\n",
    "#random forest model with 100 estimators\n",
    "my_forest = RandomForestClassifier(class_weight='balanced',\n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=10,\n",
    "                                  min_samples_split=50,                        \n",
    "                                  n_jobs=-1,\n",
    "                                  random_state = 48)\n",
    "\n",
    "score = cross_val_score(my_forest, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99585cc",
   "metadata": {},
   "source": [
    "Unsuprisingly, our models performance decreases significantly. We are now looking at a score of 0.57 with a 95% CI of between 0.53 and 0.62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b1888",
   "metadata": {},
   "source": [
    "# RF Model Including TFIDF Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d747bd6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:26.186610Z",
     "start_time": "2022-01-07T19:54:24.243808Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6070745341954478 0.5541786414143866\n",
      "0.5806265878049172\n"
     ]
    }
   ],
   "source": [
    "#RF model with top 10 TFIDF terms \n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit_tfidf_10.drop(columns = drop)\n",
    "y = reddit_tfidf_10['target']\n",
    "\n",
    "my_forest = RandomForestClassifier(class_weight='balanced',\n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=10,\n",
    "                                  min_samples_split=50,                        \n",
    "                                  n_jobs=-1,\n",
    "                                  random_state=48)\n",
    "\n",
    "score = cross_val_score(my_forest, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d46a4",
   "metadata": {},
   "source": [
    "Here we see that including the TFIDF terms does not really change the performance of our model. The CI narrows ever so slightly. Next we will check what happens when including more TFIDF terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c56769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:28.524357Z",
     "start_time": "2022-01-07T19:54:26.189603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6094494395890014 0.5560216622071247\n",
      "0.5827355508980631\n"
     ]
    }
   ],
   "source": [
    "#RF model with top 100 TFIDF terms\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit_tfidf_100.drop(columns = drop)\n",
    "y = reddit_tfidf_100['target']\n",
    "\n",
    "my_forest = RandomForestClassifier(class_weight='balanced',\n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=10,\n",
    "                                  min_samples_split=50,                        \n",
    "                                  n_jobs=-1,\n",
    "                                  random_state=48)\n",
    "\n",
    "score = cross_val_score(my_forest, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00b8a8",
   "metadata": {},
   "source": [
    "Again, we see that including more TFIDF terms does not improve the performance of our model. I believe this to be due to the fact that there are so many different subreddits, with so many different focuses, and titles are very short. On average, the titles in our data set are only 10 words long. For this reason, I will be choosing to exclude titles from my random forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd711cb",
   "metadata": {},
   "source": [
    "# Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedc9a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:33.119068Z",
     "start_time": "2022-01-07T19:54:28.527350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6141428798201969 0.5475313473811785\n",
      "0.5808371136006877\n"
     ]
    }
   ],
   "source": [
    "#logistic Regression without TFIDF terms\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'stemmed_title', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit.drop(columns = drop)\n",
    "y = reddit['target']\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=-1)\n",
    "score = cross_val_score(logreg, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "321da1a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:38.467761Z",
     "start_time": "2022-01-07T19:54:33.126051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6149191899516084 0.5439425924408468\n",
      "0.5794308911962276\n"
     ]
    }
   ],
   "source": [
    "#logistic regression with TFIDF terms\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit_tfidf_10.drop(columns = drop)\n",
    "y = reddit_tfidf_10['target']\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=-1)\n",
    "score = cross_val_score(logreg, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399c7a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.014549Z",
     "start_time": "2022-01-07T19:54:38.473745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6052458297820817 0.5463083386805465\n",
      "0.5757770842313141\n"
     ]
    }
   ],
   "source": [
    "#logistic regression with TFIDF terms\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit_tfidf_100.drop(columns = drop)\n",
    "y = reddit_tfidf_100['target']\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=-1)\n",
    "score = cross_val_score(logreg, X, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6b070",
   "metadata": {},
   "source": [
    "Our logistic model has a score of 0.57 with a 95% CI of between 0.52 and 0.62.\n",
    "\n",
    "Similar to our random forest model, we see that our model's performance does not change significantly with the top 10 TFIDF terms, and worsens slightly with the top 100. I see no reason to include them given the current state of our model. This indicates that, given our data set, there are not any specific key words to include that will improve or hurt your chances of having a successful post. I believe this would change if we looked at data across a wider time frame, as posts related to current events are more likely to succeed, and all of our data was scraped over the course of a few weeks around the winter holidays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cff68",
   "metadata": {},
   "source": [
    "# Lasso Regression (Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47199c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.206036Z",
     "start_time": "2022-01-07T19:54:49.020535Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=array([1.00000000e-10, 1.59985872e-10, 2.55954792e-10, 4.09491506e-10,\n",
       "       6.55128557e-10, 1.04811313e-09, 1.67683294e-09, 2.68269580e-09,\n",
       "       4.29193426e-09, 6.86648845e-09, 1.09854114e-08, 1.75751062e-08,\n",
       "       2.81176870e-08, 4.49843267e-08, 7.19685673e-08, 1.15139540e-07,\n",
       "       1.84206997e-07, 2.94705170e-07, 4.71486636e-07, 7.54312006e-07,\n",
       "       1.20679264e-06, 1.93069773e-0...\n",
       "       5.17947468e-05, 8.28642773e-05, 1.32571137e-04, 2.12095089e-04,\n",
       "       3.39322177e-04, 5.42867544e-04, 8.68511374e-04, 1.38949549e-03,\n",
       "       2.22299648e-03, 3.55648031e-03, 5.68986603e-03, 9.10298178e-03,\n",
       "       1.45634848e-02, 2.32995181e-02, 3.72759372e-02, 5.96362332e-02,\n",
       "       9.54095476e-02, 1.52641797e-01, 2.44205309e-01, 3.90693994e-01,\n",
       "       6.25055193e-01, 1.00000000e+00]),\n",
       "        cv=5, max_iter=50000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso Regression with standard scaled variables\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'stemmed_title', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "X = reddit.drop(columns = drop)\n",
    "y = reddit['target']\n",
    "\n",
    "#scaling the data\n",
    "sc = StandardScaler()\n",
    "Z = sc.fit_transform(X)\n",
    "\n",
    "#creating a list of possible alphas\n",
    "l_alphas = np.logspace(-10, 0, 50)\n",
    "\n",
    "#testing for the optimal alpha\n",
    "lasso_cv = LassoCV(alphas = l_alphas, cv = 5, max_iter = 50000)\n",
    "\n",
    "lasso_cv.fit(Z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53586a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.221993Z",
     "start_time": "2022-01-07T19:54:49.207033Z"
    }
   },
   "outputs": [],
   "source": [
    "var_coefs = list(zip(X.columns, list(lasso_cv.coef_)))\n",
    "\n",
    "#creating a list of variables to drop\n",
    "to_drop = []\n",
    "for item_pair in var_coefs:\n",
    "    if item_pair[1] == 0:\n",
    "        to_drop.append(item_pair[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b5dd3b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.237951Z",
     "start_time": "2022-01-07T19:54:49.222990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contains_video',\n",
       " 'posted_hour_7',\n",
       " 'posted_hour_8',\n",
       " 'posted_hour_12',\n",
       " 'posted_hour_13',\n",
       " 'posted_hour_14',\n",
       " 'posted_hour_17',\n",
       " 'posted_hour_21',\n",
       " 'posted_hour_22',\n",
       " 'posted_weekday_Monday',\n",
       " 'posted_weekday_Thursday',\n",
       " 'posted_weekday_Wednesday']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3228af3",
   "metadata": {},
   "source": [
    "Our lasso regression model zeros out several variables that do not have a significant impact on a post's chances at suceeding. Namely we see that whether or not a post contains a video does not have a significant impact, a list of hours that do not have a significant impact, and that monday and wednesday do not have a significant impact. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fecd06",
   "metadata": {},
   "source": [
    "# Final Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7845ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.381566Z",
     "start_time": "2022-01-07T19:54:49.239946Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6254402508496129 0.5560582294273337\n",
      "0.5907492401384733\n"
     ]
    }
   ],
   "source": [
    "#creating a regression with the reduced dataframe\n",
    "\n",
    "#columns to drop for modeling\n",
    "drop = ['title', 'hours_ago', 'scraped_time', 'target', 'posted_time', \n",
    "        'post_id', 'comments', 'stemmed_title', 'subreddit', 'upvotes']\n",
    "\n",
    "#creating our X and y\n",
    "\n",
    "X = X.drop(columns = to_drop)\n",
    "sc = StandardScaler()\n",
    "Z = sc.fit_transform(X)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "score = cross_val_score(logreg, Z, y)\n",
    "\n",
    "score = cross_val_score(logreg, Z, y)\n",
    "print(np.mean(score)+2*np.std(score), np.mean(score)-2*np.std(score))\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182c718",
   "metadata": {},
   "source": [
    "Our final model has a score of 0.57 with a 95% CI of between 0.53 and 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1987328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.461353Z",
     "start_time": "2022-01-07T19:54:49.382564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.01196837, 0.00997353, 0.01396251, 0.00897574, 0.01097083]),\n",
       " 'score_time': array([0.00099683, 0.00099707, 0.0009985 , 0.00099683, 0.        ]),\n",
       " 'test_score': array([0.59205903, 0.55852373, 0.60351494, 0.59156415, 0.60808436])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(logreg, Z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a081e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.525182Z",
     "start_time": "2022-01-07T19:54:49.462351Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#selecting a model to examine the coefficients\n",
    "#we select the third model as it has the score closest to the mean\n",
    "final_results = cross_validate(logreg, Z, y, return_estimator=True)\n",
    "final_logreg = final_results['estimator'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529f8a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T19:54:49.541140Z",
     "start_time": "2022-01-07T19:54:49.526180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_only</td>\n",
       "      <td>0.311264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title_len</td>\n",
       "      <td>0.165071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>posted_hour_18</td>\n",
       "      <td>0.045607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>posted_hour_10</td>\n",
       "      <td>0.041421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>posted_hour_11</td>\n",
       "      <td>0.038970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>posted_hour_20</td>\n",
       "      <td>0.038668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>posted_hour_19</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>posted_hour_16</td>\n",
       "      <td>0.024141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>posted_hour_9</td>\n",
       "      <td>0.022530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>posted_weekday_Sunday</td>\n",
       "      <td>0.009033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>posted_hour_15</td>\n",
       "      <td>-0.004319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>posted_weekday_Tuesday</td>\n",
       "      <td>-0.014285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>posted_weekday_Friday</td>\n",
       "      <td>-0.024283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contains_image</td>\n",
       "      <td>-0.032171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title_emoji</td>\n",
       "      <td>-0.045207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>posted_hour_23</td>\n",
       "      <td>-0.055937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>posted_hour_24</td>\n",
       "      <td>-0.059976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>posted_weekday_Saturday</td>\n",
       "      <td>-0.062351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>posted_hour_1</td>\n",
       "      <td>-0.078599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>posted_hour_2</td>\n",
       "      <td>-0.084328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Variable  Coefficient\n",
       "1                 text_only     0.311264\n",
       "2                 title_len     0.165071\n",
       "15           posted_hour_18     0.045607\n",
       "11           posted_hour_10     0.041421\n",
       "12           posted_hour_11     0.038970\n",
       "17           posted_hour_20     0.038668\n",
       "16           posted_hour_19     0.036068\n",
       "14           posted_hour_16     0.024141\n",
       "10            posted_hour_9     0.022530\n",
       "22    posted_weekday_Sunday     0.009033\n",
       "13           posted_hour_15    -0.004319\n",
       "23   posted_weekday_Tuesday    -0.014285\n",
       "20    posted_weekday_Friday    -0.024283\n",
       "0            contains_image    -0.032171\n",
       "3               title_emoji    -0.045207\n",
       "18           posted_hour_23    -0.055937\n",
       "19           posted_hour_24    -0.059976\n",
       "21  posted_weekday_Saturday    -0.062351\n",
       "4             posted_hour_1    -0.078599\n",
       "5             posted_hour_2    -0.084328"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficient_list = list(zip(X.columns, final_logreg.coef_[0]))\n",
    "coefficient_df = pd.DataFrame(zip(X.columns, final_logreg.coef_[0]))\n",
    "coefficient_df = coefficient_df.rename(columns = {0:'Variable',1:'Coefficient'})\n",
    "coefficient_df.sort_values('Coefficient', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e229f4",
   "metadata": {},
   "source": [
    "Examining our coefficients, we can determine that to give our post the best chance of succeeding, it should be a text only post, posted on a thursday, around 10-11 am. The worst things we could do for our chances of having a successful post would be posting at an image on a friday between 11pm and midnight.\n",
    "\n",
    "It should be noted that our final model does not have a lot of predictive power, but ultimately what we are trying to predict is human behaviour. We are trying to predict what people like, and how they will interact with a post. When the scope of our analysis is the front page, it is very difficult to determine what will succeed. I don't even have a strong understanding of the algorithm used to determine what ends up on the front page. This project would likely be much more successful if we took a deep dive into a single subreddit, and tried to determine what attributes helped a post become successful within the boundaries of that subreddit. \n",
    "\n",
    "If we were to narrow the scope of our analysis in this way, I believe that the TFIDF terms would become much more powerful predictors. For example, in the anti-work subreddit, I would imagine that posts with much more negative titles would tend to do better, as the people that are members of that subreddit love to see people talking trash about their bosses and explaining their unpleasant working conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680f4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
